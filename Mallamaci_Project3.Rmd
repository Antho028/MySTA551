---
title: "Comparative Analysis of Predictive Models for Heart Disease Classification"
author: "Anthony Mallamaci"
date: "2025-10-14"
output:
  html_document: 
    toc: yes
    toc_float: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    fig_caption: yes
    number_sections: yes
    toc_collapsed: yes
    code_folding: hide
    code_download: yes
    smooth_scroll: yes
    theme: lumen
  word_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    keep_md: yes
  pdf_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
    fig_width: 5
    fig_height: 4
---

```{=html}
<style type="text/css">

div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}

h1.title {
  font-size: 20px;
  font-weight: bold;
  color: DarkRed;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
    font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}
h1 { /* Header 3 - and the author and data headers use this too  */
    font-size: 22px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: center;
}
h2 { /* Header 3 - and the author and data headers use this too  */
    font-size: 18px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - and the author and data headers use this too  */
    font-size: 16px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - and the author and data headers use this too  */
    font-size: 14px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}
</style>
```
```{r setup, include=FALSE}
# code chunk specifies whether the R code, warnings, and output 
# will be included in the output files.
if (!require("ggplot2")) {
   install.packages("ggplot2")
   library(ggplot2)
}
if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("ISLR")) {
   install.packages("ISLR")
   library(ISLR)
}
if (!require("neuralnet")) {
   install.packages("neuralnet")
   library(neuralnet)
}
if (!require("caret")) {
   install.packages("caret")
   library(caret)
}
if (!require("nnet")) {
   install.packages("nnet")
   library(nnet)
}
if (!require("haven")) {
   install.packages("haven")
   library(haven)
}
if(!require(klaR)){
  install.packages("klaR")
  library(klaR)
}
if (!require("pander")) {
   install.packages("pander")
   library(pander)
}
  if (!require("dbscan")) {
   install.packages("dbscan")
   library(dbscan)
}
if (!require("dbscan")) {
   install.packages("dbscan")
   library(dbscan)
}
if (!require("pROC")) {
   install.packages("pROC")
   library(pROC)
}
if (!require("gridExtra")) {
   install.packages("gridExtra")
   library(gridExtra)
}
if (!require("ggparallel")) {
   install.packages("ggparallel")
   library(ggparallel)
}
# The following R source code will be use to plot the path plot
# of neural network model with estimated weights from the data.
#source("")
# knitr::opts_knit$set(root.dir = "C:\\STA551\\w08")

knitr::opts_chunk$set(echo = TRUE,       
                      warning = FALSE,   
                      results = TRUE,   
                      message = FALSE,
                      comment= NA)
```

<h1 style="color:black;">Introduction</h1>

This dataset contains information related to credit card applications and is designed for analysis and predictive modeling. It includes customer demographic features, financial attributes, and past credit history. The goal is to determine which factors influence whether a credit card application is approved.

<h1 style="color:black;">EDA</h1>

This section explores the dataset through summary statistics and visualizations to understand key variable distributions and identify potential patterns related to heart disease.

```{r }

url <- "https://raw.githubusercontent.com/Antho028/MySTA551/refs/heads/main/Credit_Card_Applications.csv"
mydata <- read.csv(url)
head(mydata)
summary(mydata)

```

The dataset contains 16 variables representing customer characteristics, with numeric features spanning a wide range of scales. Several attributes (such as A2 and A3) show moderate variation, while others like A14 exhibit extremely large variability, indicating the presence of high-value outliers. The Class variable is slightly imbalanced, with approximately 44% of customers labeled as Class 1 and 56% as Class 0.

```{r }

# --- Pairwise scatterplot ---
# Select numeric variables for scatterplot
NumVar <- mydata[, c(
  "A2","A3","A5","A6","A7","A10","A13","A14"
)]

# Basic pairwise scatterplot
pairs(
  NumVar,
  pch = 16,
  cex = 0.5,
  col = rgb(0, 0, 0.5, 0.05),
  main = "Pairwise Scatterplot of Numeric Variables"
)

# Color by churn (1 = churned)
cols <- ifelse(mydata$Class == 1,
               rgb(1, 0, 0, 0.3),
               rgb(0, 0, 0.5, 0.3))

pairs(
  NumVar,
  pch = 16,
  cex = 0.5,
  col = cols,
  main = "Pairwise Scatterplot Colored by Churn"
)


```
The pairwise scatterplots reveal several important structural patterns in the numeric features. Most variables show weak linear relationships, with many forming vertical or horizontal bands which indicates that multiple features are discrete or take on only a limited set of values (e.g., A5, A6, A10). A few variables, such as A2 with A7 and A3 with A7, show mild positive trends, suggesting that increases in general activity or spending may be associated across attributes. The extremely large scale of A14 produces a highly skewed distribution with only a handful of very large values, confirming the presence of strong outliers.

```{r }
num_vars <- c(
  "A2","A3","A4","A5","A6","A7","A10", "A12","A13","A14"
)

par(mfrow = c(3, 4))

for (v in num_vars) {
  hist(mydata[[v]],
       main = paste("Distribution of", v),
       xlab = v,
       col = "steelblue",
       border = "white")
}

cat_vars <- c("A1", "A8", "A9", "A11","Class")

par(mfrow = c(1, 2))

for (v in cat_vars) {
  tbl <- table(mydata[[v]])
  barplot(tbl,
          main = paste("Distribution of", v),
          col = "steelblue",
          border = "white",
          ylab = "Count",
          las = 2)
}

```

The histograms reveal that most numeric variables in the dataset are highly right-skewed, with long tails extending toward larger values. Variables such as A2, A3, A5, A7, and A10 show a concentration of observations at low or moderate values, followed by a rapid decline in frequency, indicating that unusually high values are relatively uncommon. Several features, including A4, A6, and A12, take on only a small number of distinct values, suggesting these may be ordinal or discretized categorical inputs rather than continuous measurements.


<h1 style="color:black;">Model Building</h1>

To prepare the dataset for modeling, we first conducted exploratory data analysis (EDA) to understand the structure, distributions, and relationships among the variables, followed by basic feature engineering to clean and organize the data for downstream analysis. After establishing a reliable analytical dataset, we applied several unsupervised machine learning techniques to extract informative patterns and reduce dimensionality. Principal Component Analysis (PCA) was used to uncover the dominant sources of variation and generate compact feature representations. Clustering methods were then employed to identify natural groupings and behavioral segments within the data. Finally, the Local Outlier Factor (LOF) algorithm was used to detect anomalous observations that may influence modeling or represent meaningful deviations in customer behavior. Together, these methods provide a structured foundation for understanding the dataset and enhancing model performance in subsequent supervised learning tasks.




```{r }

num_data <- mydata[sapply(mydata, is.numeric)]

# Compute correlation matrix
cor_matrix <- cor(num_data, use = "pairwise.complete.obs")
print(cor_matrix)

# Identify pairs with abs(correlation) > 0.7
high_corr <- which(abs(cor_matrix) > 0.7 & abs(cor_matrix) < 1, arr.ind = TRUE)

# Print readable results
if (nrow(high_corr) == 0) {
  cat("No numeric variables have correlation above 0.7\n")
} else {
  for (i in 1:nrow(high_corr)) {
    var1 <- rownames(cor_matrix)[high_corr[i, 1]]
    var2 <- colnames(cor_matrix)[high_corr[i, 2]]
    value <- cor_matrix[high_corr[i, 1], high_corr[i, 2]]
    cat(var1, "and", var2, "have correlation =", round(value, 3), "\n")
  }
}


```



```{r }

# Convert to proper data frame
mydata <- as.data.frame(mydata)

# Keep only numeric columns
num_vars <- mydata[, sapply(mydata, is.numeric), drop = FALSE]

# Check for NA, Inf, -Inf
colSums(!is.finite(as.matrix(num_vars)))

# Remove rows with NA or Inf
num_vars_clean <- num_vars[
  complete.cases(num_vars) &
    apply(num_vars, 1, function(x) all(is.finite(x))),
]

# Run PCA
pca_model <- prcomp(num_vars_clean, center = TRUE, scale = TRUE)

# PCA results
summary(pca_model)
round(pca_model$rotation, 2)

# Scree plot
screeplot(pca_model, type = "lines", main = "Scree Plot")

# Add first two PCs back to cleaned dataset
final_data <- mydata[rownames(num_vars_clean), ]
final_data$PC1 <- pca_model$x[,1]
final_data$PC2 <- pca_model$x[,2]


```
<h2 style="color:black;">Principle Component Analysis (PCA)</h2>

Given that the dataset contains multiple numerical variables with different scales, skewed distributions, and potential redundancy, Principal Component Analysis (PCA) was used to uncover the main patterns of variation and create a more compact representation of the data. PCA was applied to the nine continuous features (A2, A3, A5, A6, A7, A10, A12, A13, and A14) after centering and scaling them to ensure equal contribution across variables with vastly different ranges. The resulting components summarize the underlying structure of the dataset, allowing us to identify which combinations of variables explain most of the variability. Scree plots and component loadings were examined to determine how many components to retain, and the first two principal components were added back to the dataset for use in clustering and later modeling. This step provides a cleaner feature space, reduces noise from highly skewed variables, and helps reveal latent behavioral dimensions within the customer population.


```{r }

num_vars <- mydata[, c("A2","A3","A5","A6","A7","A10","A12","A13","A14")]

# Check for missing or infinite values
colSums(!is.finite(as.matrix(num_vars)))  # should all be 0

# Perform PCA (center and scale)
pca_result <- prcomp(num_vars, center = TRUE, scale. = TRUE)

# Summary: standard deviation, proportion of variance, cumulative proportion
summary(pca_result)

# Scree plot to visualize variance explained
screeplot(pca_result, type = "lines", main = "Scree Plot of PCA")

# Factor loadings (which variables contribute most to each PC)
round(pca_result$rotation, 2)

# Extract the PC scores (transformed values of original data)
pc_scores <- pca_result$x
head(pc_scores[, 1:2])  # first two PCs

# Optionally, add first two PCs to the original dataset
mydata$PC1 <- pc_scores[,1]
mydata$PC2 <- pc_scores[,2]

# Save the new dataset with PCs
write.csv(mydata, "New_Data_with_PCs.csv", row.names = FALSE)


```
<h2 style="color:black;">PCA Conclusion</h2>

The scree plot indicates that the first two principal components dominate the variance structure, explaining 22.7% (PC1) and 16.0% (PC2) of the total variance. The clear drop after PC2 suggests retaining two components.PC1 shows the strongest loadings on A7, A3, and A10, indicating these variables contribute most to the variation captured by PC1.PC2 loads positively on A5, A6, and A13, and negatively on A2 and A3, meaning PC2 contrasts one group of variables (A5, A6, A13) against another (A2, A3). Unfortunately since the dataset does not provide definitions for these attributes, we can only interpret the components strictly based on statistical structure, not apply any real-world interpretations.





<h2 style="color:black;">Clustering</h2>

Clustering was used as an unsupervised learning approach to explore natural groupings within the dataset based solely on similarity among the numeric features. After preparing the data through scaling and dimensionality reduction with PCA, clustering algorithms were applied to identify patterns or segments that may not be visible through traditional analysis. This process helps reveal underlying structure in the customer population and provides additional feature representations that can be incorporated into subsequent modeling.

<h3 style="color:black;">K-Means Clustering</h3>

K-means clustering is an unsupervised learning technique that partitions observations into a chosen number of clusters based on similarity. The algorithm works by iteratively assigning data points to the closest cluster center and then updating those centers to minimize the within-cluster variation. In this project, k-means was applied to the standardized scores of the first three principal components (PC1–PC3) to ensure that differences in scale did not bias the distance calculations. The elbow method and silhouette analysis were used to guide the selection of an appropriate number of clusters, after which the final model was fitted. Cluster memberships were then visualized in PCA space to examine how the groups formed within the reduced-dimensional representation of the data.

```{r }
clust.data <- pc_scores[, c("PC1", "PC2", "PC3")]
scaled.data <- scale(clust.data)


wss <- NULL
K <- 15
for (i in 1:K){
  wss[i] <- kmeans(scaled.data, centers = i, nstart = 25)$tot.withinss
}

plot(1:K, wss, type="b",
     xlab="Number of Clusters",
     ylab="WSS",
     main="Elbow Plot")

library(factoextra)
fviz_nbclust(scaled.data, kmeans, method="silhouette")

# Use first 3 PCs
clust.data <- as.data.frame(pca_result$x[, 1:3])

# Scale before K-means
scaled.data <- scale(clust.data)

# Choose k = 7 based on silhouette
set.seed(123)
k_result <- kmeans(scaled.data, centers = 7, nstart = 50, iter.max = 100)

clust.data$cluster <- as.factor(k_result$cluster)

# Plot clusters
plot(clust.data$PC1, clust.data$PC2,
     col = clust.data$cluster,
     pch = 19,
     xlab = "PC1", ylab = "PC2",
     main = "Clusters on PCA (PC1 vs PC2)")

legend("topright",
       legend = levels(clust.data$cluster),
       col = 1:length(levels(clust.data$cluster)),
       pch = 19)



```
<h3 style="color:black;">K-Means Clustering Conclusion</h3>

The clustering analysis organizes the data into seven distinct groups based on the selected LOF variables, indicating that individuals in the dataset naturally separate into several meaningful patterns of similarity. The clusters reflect underlying differences in the numerical features, such as higher or lower LOF scores, that cause some observations to group closely together while others stand apart. The PCA plots show that most of the variation in the data can be captured by the first few principal components, which means the dataset has clear dominant dimensions that drive these groupings. Together, the cluster plot, PCA visualization, and other diagnostic charts suggest that the dataset contains well-defined subgroups, each representing different profiles or patterns across the LOF measures.



<h3 style="color:black;">Hierarchical Clustering</h3>

Hierarchical clustering is an unsupervised learning method that builds a tree-like structure (dendrogram) to represent how observations group together based on similarity. Unlike k-means, it does not require specifying the number of clusters upfront; instead, it iteratively merges the closest clusters using a chosen distance metric and linkage method. In this analysis, Euclidean distance and complete linkage were applied to the standardized principal component scores to create a hierarchical cluster tree. The dendrogram, along with the elbow and silhouette criteria, guided the selection of an appropriate cut level, and the resulting cluster assignments were visualized in the PCA space to illustrate how the hierarchical structure maps onto the reduced-dimensional representation of the data.

```{r }

distance <- dist(scaled.data, method="euclidean")

hc <- hclust(distance, method="complete")
plot(hc, cex=.6, hang=-1, main="Dendrogram")

fviz_nbclust(scaled.data, FUN = hcut, method = "wss")
fviz_nbclust(scaled.data, FUN = hcut, method = "silhouette")

group <- cutree(hc, k=4)
clust.data$HCgroup <- group

plot(clust.data$PC1, clust.data$PC2,
     pch = 19,
     col = factor(clust.data$HCgroup),
     xlab ="PC1",
     ylab = "PC2",
     main = "Hierarchical Clustering")





```
<h3 style="color:black;">Hierarchical Clustering Conclusion</h3>

The hierarchical clustering analysis reveals that the dataset naturally forms a small number of broad groups rather than many sharply separated clusters. The dendrogram shows long vertical merges, indicating that observations are generally similar to one another and only become distinguishable at higher distances. Both the WSS and silhouette methods suggest that around four clusters provide a reasonable balance between compactness and separation.

When these four groups are plotted on the PCA space, the clusters appear partially overlapping, which means the differences between groups are subtle rather than sharply defined. One cluster is more spread along the positive PC1 direction, while the others are concentrated near the center of the PCA plot. This suggests that the strongest separation is driven by only one or two underlying dimensions, with most observations sharing similar feature patterns.

Overall, hierarchical clustering indicates that the dataset contains a few loosely defined subgroups, with moderate separation along the principal components. The structure is not strongly partitioned, but there are identifiable regions in the data where observations tend to cluster together based on their similarity in the selected numerical attributes.


<h2 style="color:black;">Local Outlier Factor (LOF)</h2>

The Local Outlier Factor (LOF) algorithm was used to identify observations whose behavior deviates significantly from that of their nearest neighbors. Unlike global distance-based outlier detection, LOF evaluates local density, allowing it to capture subtle anomalies even when different regions of the dataset have varying levels of sparsity. In this analysis, LOF scores were computed using the primary numeric features to flag unusually high- or low-density points that may represent rare customer behaviors or potential data irregularities. The distribution of LOF scores, detection rates across different thresholds, and the ROC curve were examined to understand how well outlierness aligns with the Class variable and to evaluate LOF’s usefulness as an additional feature for downstream modeling.

```{r }
library(dbscan)      # for LOF
library(pROC)      # for ROC curves

lof.Churn.100 <- lof(
  mydata[, c("A2","A3","A4","A5","A6","A7","A10","A11","A12","A13","A14")],
  k = 100
)

#summary(lof.Churn.100)


### 1. Your outcome variable (replace Class if needed)
num_cols <- c("A2","A3","A4","A5","A6","A7","A10","A11","A12","A13","A14")
mydata_numeric <- mydata[, num_cols]

# Remove rows with any NA in these variables
complete_rows <- complete.cases(mydata_numeric, mydata$Class)

mydata_numeric <- mydata_numeric[complete_rows, ]
category <- mydata$Churn[complete_rows]

### 3. Compute Local Outlier Factor
lof_scores <- dbscan::lof(mydata_numeric, k = 100)

### 4. Plot LOF distribution (excluding giant outliers)
hist(lof_scores[lof_scores < 5],
     breaks = 50,
     main = "LOF Distribution",
     xlab = "LOF Score")

### 5. Catching rate across thresholds
cut.lof <- seq(1, 5, length = 21)
catching_rate <- numeric(length(cut.lof))

for(i in 1:length(cut.lof)){
  ID <- lof_scores > cut.lof[i]
  fraud_sub <- mydata$Class[ID]
  catching_rate[i] <- sum(fraud_sub) / sum(ID)
}

default_rate <- sum(mydata$Class) / nrow(mydata)
relative_detect_rate <- (catching_rate - default_rate) / default_rate

### 6. ROC Curve
category <- as.character(mydata$Class)

#install.packages("Rlof")

# Compute LOF scores (k = 100)
lof_scores <- Rlof::lof(mydata_numeric, k = 100)

#length(category)
#length(lof_scores)

library(pROC)

category <- factor(category, levels = c(1, 0))

ROCobj <- roc(category, lof_scores, direction = ">")

plot(1 - ROCobj$specificities,
     ROCobj$sensitivities,
     type = "l", lwd = 2, col = "blue",
     xlab = "1 - Specificity", ylab = "Sensitivity",
     main = "ROC Curve")

segments(0,0,1,1,col="red",lty=2)


```

<h2 style="color:black;">Local Outlier Factor Conclusion</h2>

The LOF analysis identifies how unusual or “outlier-like” each observation is relative to its neighbors in the high-dimensional feature space. Most LOF scores in this dataset fall just above 1, which is expected because a value of 1 indicates typical, non-outlier behavior. The distribution is strongly right-skewed: a large majority of observations cluster between 1.0 and 1.5, while a small number extend into very high LOF values, including extreme outliers above 5 and even one exceeding 60. This indicates the presence of a few observations that behave significantly differently from the rest of the dataset.

When LOF is used as a predictor for the Class label, the ROC curve shows performance moderately above random chance but not strong enough for reliable classification. The curve rises above the diagonal baseline, suggesting that higher LOF scores are associated with a higher probability of being in the positive (Class 1) category, but the separation is weak. This means LOF captures some signal about abnormal behavior related to the outcome, but most observations labeled as positive are not extreme outliers, and many high-LOF cases belong to the negative class.

Overall, LOF reveals that the dataset contains few true structural outliers, and while outlierness correlates slightly with the outcome, LOF alone is not an effective classifier. It is better used as an additional feature to highlight unusual patterns rather than as a standalone prediction method.




<h1 style="color:black;">LDA</h1>

Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction and classification method that finds linear combinations of predictors which best separate predefined classes. Unlike PCA, which ignores the outcome, LDA explicitly uses the Class label to estimate a direction (or directions) in feature space that maximizes the ratio of between-class variance to within-class variance, improving discriminative power.

In this project, LDA was applied using the numerical predictors A2, A3, A4, A5, A6, A7, A10, A11, A12, A13, and A14 to distinguish between the two Class categories (0 = not approved, 1 = approved). The data was first split into training and test sets, an LDA model was fit on the training data, and then predictions, class posterior probabilities, and LDA scores were obtained for the test set. This approach provides both a compact, supervised feature representation (the LDA component) and a baseline classification model to compare against models built using PCA features and other engineered variables.

```{r }

set.seed(123)  # for reproducibility

training_sample <- sample(c(TRUE, FALSE),
                          nrow(mydata),
                          replace = TRUE,
                          prob = c(0.6, 0.4))

train <- mydata[training_sample, ]
test  <- mydata[!training_sample, ]

library(klaR)
lda.model <- lda(Class ~ A2 + A3 + A4 + A5 + A6 + A7 + 
                 A10 + A11 + A12 + A13 + A14, data = train)

lda.pred <- predict(lda.model, newdata = test)
test$lda_class <- lda.pred$class


lda.model        # or just print(lda.model)
lda.model$scaling

cm <- table(Predicted = test$lda_class,
            Actual    = test$Class)
cm

# Accuracy
accuracy <- sum(diag(cm)) / sum(cm)
accuracy



```

<h1 style="color:black;">LDA Conclusion</h1>

Linear Discriminant Analysis (LDA) was applied to the credit approval dataset to create a supervised feature extraction that maximizes separation between approved and rejected applications. Because the dataset contains two classes, LDA produced a single discriminant component (LD1). The group means showed noticeable differences between classes across several variables, and the LD1 coefficients indicated that features such as A4, A12, A5, and A10 contributed most strongly to class separation.

Using LD1 as the classification rule, LDA achieved an overall accuracy of approximately 81%, correctly identifying most observations in the test set. The confusion matrix demonstrated that LDA performed well for both classes, reflecting its ability to extract a direction that captures meaningful discriminatory information from the numerical attributes. Overall, LDA provided an effective supervised dimensionality‐reduction method for this dataset and produced a useful, compact feature for predicting credit approval outcomes.



<h1 style="color:black;">PCA vs LDA</h1>

PCA and LDA produced distinctly different insights into the credit approval data. PCA, as an unsupervised method, extracted components that explained the major sources of variance in the numerical features, but these components did not strongly separate approved and rejected applications. In contrast, LDA used the Class label to identify a single discriminant direction (LD1) that maximized class separation, achieving an accuracy of approximately 81% on the test set. The LDA coefficients showed that variables such as A4, A12, A5, and A10 contributed most to the discriminatory power of the model. While PCA is more appropriate for exploratory analysis and dimensionality reduction, LDA proved more effective for prediction in this context because it explicitly optimizes for classification rather than variance. Together, both methods provide complementary perspectives on the dataset—PCA reveals underlying structure, while LDA enhances predictive performance.

