---
title: "Comparative Analysis of Predictive Models for Heart Disease Classification"
author: "Anthony Mallamaci"
date: "2025-10-14"
output:
  html_document: 
    toc: yes
    toc_float: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    fig_caption: yes
    number_sections: yes
    toc_collapsed: yes
    code_folding: hide
    code_download: yes
    smooth_scroll: yes
    theme: lumen
  word_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    keep_md: yes
  pdf_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
    fig_width: 5
    fig_height: 4
---

```{=html}
<style type="text/css">

div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}

h1.title {
  font-size: 20px;
  font-weight: bold;
  color: DarkRed;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
    font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}
h1 { /* Header 3 - and the author and data headers use this too  */
    font-size: 22px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: center;
}
h2 { /* Header 3 - and the author and data headers use this too  */
    font-size: 18px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - and the author and data headers use this too  */
    font-size: 16px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - and the author and data headers use this too  */
    font-size: 14px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}
</style>
```
```{r setup, include=FALSE}
# code chunk specifies whether the R code, warnings, and output 
# will be included in the output files.
if (!require("ggplot2")) {
   install.packages("ggplot2")
   library(ggplot2)
}
if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("ISLR")) {
   install.packages("ISLR")
   library(ISLR)
}
if (!require("neuralnet")) {
   install.packages("neuralnet")
   library(neuralnet)
}
if (!require("caret")) {
   install.packages("caret")
   library(caret)
}
if (!require("nnet")) {
   install.packages("nnet")
   library(nnet)
}
if (!require("haven")) {
   install.packages("haven")
   library(haven)
}
if(!require(klaR)){
  install.packages("klaR")
  library(klaR)
}
if (!require("pander")) {
   install.packages("pander")
   library(pander)
}
  if (!require("dbscan")) {
   install.packages("dbscan")
   library(dbscan)
}
if (!require("dbscan")) {
   install.packages("dbscan")
   library(dbscan)
}
if (!require("pROC")) {
   install.packages("pROC")
   library(pROC)
}
if (!require("gridExtra")) {
   install.packages("gridExtra")
   library(gridExtra)
}
if (!require("ggparallel")) {
   install.packages("ggparallel")
   library(ggparallel)
}
# The following R source code will be use to plot the path plot
# of neural network model with estimated weights from the data.
#source("")
# knitr::opts_knit$set(root.dir = "C:\\STA551\\w08")

knitr::opts_chunk$set(echo = TRUE,       
                      warning = FALSE,   
                      results = TRUE,   
                      message = FALSE,
                      comment= NA)
```

<h1 style="color:black;">Introduction</h1>

This dataset contains information related to credit card applications and is designed for analysis and predictive modeling. It includes customer demographic features, financial attributes, and past credit history. The goal is to determine which factors influence whether a credit card application is approved.

<h1 style="color:black;">EDA</h1>

This section explores the dataset through summary statistics and visualizations to understand key variable distributions and identify potential patterns related to heart disease.

```{r }

url <- "https://raw.githubusercontent.com/Antho028/MySTA551/refs/heads/main/Credit_Card_Applications.csv"
mydata <- read.csv(url)
head(mydata)
summary(mydata)

```

The dataset contains 16 variables representing customer characteristics, with numeric features spanning a wide range of scales. Several attributes (such as A2 and A3) show moderate variation, while others like A14 exhibit extremely large variability, indicating the presence of high-value outliers. The Class variable is slightly imbalanced, with approximately 44% of customers labeled as Class 1 and 56% as Class 0.

```{r }

# --- Pairwise scatterplot ---
# Select numeric variables for scatterplot
NumVar <- mydata[, c(
  "A2","A3","A5","A6","A7","A10","A13","A14"
)]

# Basic pairwise scatterplot
pairs(
  NumVar,
  pch = 16,
  cex = 0.5,
  col = rgb(0, 0, 0.5, 0.05),
  main = "Pairwise Scatterplot of Numeric Variables"
)

# Color by churn (1 = churned)
cols <- ifelse(mydata$Class == 1,
               rgb(1, 0, 0, 0.3),
               rgb(0, 0, 0.5, 0.3))

pairs(
  NumVar,
  pch = 16,
  cex = 0.5,
  col = cols,
  main = "Pairwise Scatterplot Colored by Churn"
)


```
The pairwise scatterplots reveal several important structural patterns in the numeric features. Most variables show weak linear relationships, with many forming vertical or horizontal bands which indicates that multiple features are discrete or take on only a limited set of values (e.g., A5, A6, A10). A few variables, such as A2 with A7 and A3 with A7, show mild positive trends, suggesting that increases in general activity or spending may be associated across attributes. The extremely large scale of A14 produces a highly skewed distribution with only a handful of very large values, confirming the presence of strong outliers.

```{r }
num_vars <- c(
  "A2","A3","A4","A5","A6","A7","A10", "A12","A13","A14"
)

par(mfrow = c(3, 4))

for (v in num_vars) {
  hist(mydata[[v]],
       main = paste("Distribution of", v),
       xlab = v,
       col = "steelblue",
       border = "white")
}

cat_vars <- c("A1", "A8", "A9", "A11","Class")

par(mfrow = c(1, 2))

for (v in cat_vars) {
  tbl <- table(mydata[[v]])
  barplot(tbl,
          main = paste("Distribution of", v),
          col = "steelblue",
          border = "white",
          ylab = "Count",
          las = 2)
}

```

The histograms reveal that most numeric variables in the dataset are highly right-skewed, with long tails extending toward larger values. Variables such as A2, A3, A5, A7, and A10 show a concentration of observations at low or moderate values, followed by a rapid decline in frequency, indicating that unusually high values are relatively uncommon. Several features, including A4, A6, and A12, take on only a small number of distinct values, suggesting these may be ordinal or discretized categorical inputs rather than continuous measurements.


<h1 style="color:black;">Model Building</h1>

To prepare the dataset for modeling, we first conducted exploratory data analysis (EDA) to understand the structure, distributions, and relationships among the variables, followed by basic feature engineering to clean and organize the data for downstream analysis. After establishing a reliable analytical dataset, we applied several unsupervised machine learning techniques to extract informative patterns and reduce dimensionality. Principal Component Analysis (PCA) was used to uncover the dominant sources of variation and generate compact feature representations. Clustering methods were then employed to identify natural groupings and behavioral segments within the data. Finally, the Local Outlier Factor (LOF) algorithm was used to detect anomalous observations that may influence modeling or represent meaningful deviations in customer behavior. Together, these methods provide a structured foundation for understanding the dataset and enhancing model performance in subsequent supervised learning tasks.




```{r }

num_data <- mydata[sapply(mydata, is.numeric)]

# Compute correlation matrix
cor_matrix <- cor(num_data, use = "pairwise.complete.obs")
print(cor_matrix)

# Identify pairs with abs(correlation) > 0.7
high_corr <- which(abs(cor_matrix) > 0.7 & abs(cor_matrix) < 1, arr.ind = TRUE)

# Print readable results
if (nrow(high_corr) == 0) {
  cat("No numeric variables have correlation above 0.7\n")
} else {
  for (i in 1:nrow(high_corr)) {
    var1 <- rownames(cor_matrix)[high_corr[i, 1]]
    var2 <- colnames(cor_matrix)[high_corr[i, 2]]
    value <- cor_matrix[high_corr[i, 1], high_corr[i, 2]]
    cat(var1, "and", var2, "have correlation =", round(value, 3), "\n")
  }
}


```



```{r }

# Convert to proper data frame
mydata <- as.data.frame(mydata)

# Keep only numeric columns
num_vars <- mydata[, sapply(mydata, is.numeric), drop = FALSE]

# Check for NA, Inf, -Inf
colSums(!is.finite(as.matrix(num_vars)))

# Remove rows with NA or Inf
num_vars_clean <- num_vars[
  complete.cases(num_vars) &
    apply(num_vars, 1, function(x) all(is.finite(x))),
]

# Run PCA
pca_model <- prcomp(num_vars_clean, center = TRUE, scale = TRUE)

# PCA results
summary(pca_model)
round(pca_model$rotation, 2)

# Scree plot
screeplot(pca_model, type = "lines", main = "Scree Plot")

# Add first two PCs back to cleaned dataset
final_data <- mydata[rownames(num_vars_clean), ]
final_data$PC1 <- pca_model$x[,1]
final_data$PC2 <- pca_model$x[,2]


```
<h2 style="color:black;">Principle Component Analysis (PCA)</h2>

Given that the dataset contains multiple numerical variables with different scales, skewed distributions, and potential redundancy, Principal Component Analysis (PCA) was used to uncover the main patterns of variation and create a more compact representation of the data. PCA was applied to the nine continuous features (A2, A3, A5, A6, A7, A10, A12, A13, and A14) after centering and scaling them to ensure equal contribution across variables with vastly different ranges. The resulting components summarize the underlying structure of the dataset, allowing us to identify which combinations of variables explain most of the variability. Scree plots and component loadings were examined to determine how many components to retain, and the first two principal components were added back to the dataset for use in clustering and later modeling. This step provides a cleaner feature space, reduces noise from highly skewed variables, and helps reveal latent behavioral dimensions within the customer population.


```{r }

num_vars <- mydata[, c("A2","A3","A5","A6","A7","A10","A12","A13","A14")]

# Check for missing or infinite values
colSums(!is.finite(as.matrix(num_vars)))  # should all be 0

# Perform PCA (center and scale)
pca_result <- prcomp(num_vars, center = TRUE, scale. = TRUE)

# Summary: standard deviation, proportion of variance, cumulative proportion
summary(pca_result)

# Scree plot to visualize variance explained
screeplot(pca_result, type = "lines", main = "Scree Plot of PCA")

# Factor loadings (which variables contribute most to each PC)
round(pca_result$rotation, 2)

# Extract the PC scores (transformed values of original data)
pc_scores <- pca_result$x
head(pc_scores[, 1:2])  # first two PCs

# Optionally, add first two PCs to the original dataset
mydata$PC1 <- pc_scores[,1]
mydata$PC2 <- pc_scores[,2]

# Save the new dataset with PCs
write.csv(mydata, "New_Data_with_PCs.csv", row.names = FALSE)


```
<h2 style="color:black;">PCA Conclusion</h2>

The scree plot indicates that the first two principal components dominate the variance structure, explaining 22.7% (PC1) and 16.0% (PC2) of the total variance. The clear drop after PC2 suggests retaining two components.PC1 shows the strongest loadings on A7, A3, and A10, indicating these variables contribute most to the variation captured by PC1.PC2 loads positively on A5, A6, and A13, and negatively on A2 and A3, meaning PC2 contrasts one group of variables (A5, A6, A13) against another (A2, A3). Unfortunately since the dataset does not provide definitions for these attributes, we can only interpret the components strictly based on statistical structure, not apply any real-world interpretations.





<h2 style="color:black;">Clustering</h2>

Clustering was used as an unsupervised learning approach to explore natural groupings within the dataset based solely on similarity among the numeric features. After preparing the data through scaling and dimensionality reduction with PCA, clustering algorithms were applied to identify patterns or segments that may not be visible through traditional analysis. This process helps reveal underlying structure in the customer population and provides additional feature representations that can be incorporated into subsequent modeling.

<h3 style="color:black;">K-Means Clustering</h3>

K-means clustering is an unsupervised learning technique that partitions observations into a chosen number of clusters based on similarity. The algorithm works by iteratively assigning data points to the closest cluster center and then updating those centers to minimize the within-cluster variation. In this project, k-means was applied to the standardized scores of the first three principal components (PC1â€“PC3) to ensure that differences in scale did not bias the distance calculations. The elbow method and silhouette analysis were used to guide the selection of an appropriate number of clusters, after which the final model was fitted. Cluster memberships were then visualized in PCA space to examine how the groups formed within the reduced-dimensional representation of the data.

```{r }
clust.data <- pc_scores[, c("PC1", "PC2", "PC3")]
scaled.data <- scale(clust.data)


wss <- NULL
K <- 15
for (i in 1:K){
  wss[i] <- kmeans(scaled.data, centers = i, nstart = 25)$tot.withinss
}

plot(1:K, wss, type="b",
     xlab="Number of Clusters",
     ylab="WSS",
     main="Elbow Plot")

library(factoextra)
fviz_nbclust(scaled.data, kmeans, method="silhouette")

# Use first 3 PCs
clust.data <- as.data.frame(pca_result$x[, 1:3])

# Scale before K-means
scaled.data <- scale(clust.data)

# Choose k = 7 based on silhouette
set.seed(123)
k_result <- kmeans(scaled.data, centers = 7, nstart = 50, iter.max = 100)

clust.data$cluster <- as.factor(k_result$cluster)

# Plot clusters
plot(clust.data$PC1, clust.data$PC2,
     col = clust.data$cluster,
     pch = 19,
     xlab = "PC1", ylab = "PC2",
     main = "Clusters on PCA (PC1 vs PC2)")

legend("topright",
       legend = levels(clust.data$cluster),
       col = 1:length(levels(clust.data$cluster)),
       pch = 19)



```
<h3 style="color:black;">K-Means Clustering Conclusion</h3>

The clustering analysis organizes the data into seven distinct groups based on the selected LOF variables, indicating that individuals in the dataset naturally separate into several meaningful patterns of similarity. The clusters reflect underlying differences in the numerical features, such as higher or lower LOF scores, that cause some observations to group closely together while others stand apart. The PCA plots show that most of the variation in the data can be captured by the first few principal components, which means the dataset has clear dominant dimensions that drive these groupings. Together, the cluster plot, PCA visualization, and other diagnostic charts suggest that the dataset contains well-defined subgroups, each representing different profiles or patterns across the LOF measures.



<h3 style="color:black;">Hierarchical Clustering</h3>

Hierarchical clustering is an unsupervised learning method that builds a tree-like structure (dendrogram) to represent how observations group together based on similarity. Unlike k-means, it does not require specifying the number of clusters upfront; instead, it iteratively merges the closest clusters using a chosen distance metric and linkage method. In this analysis, Euclidean distance and complete linkage were applied to the standardized principal component scores to create a hierarchical cluster tree. The dendrogram, along with the elbow and silhouette criteria, guided the selection of an appropriate cut level, and the resulting cluster assignments were visualized in the PCA space to illustrate how the hierarchical structure maps onto the reduced-dimensional representation of the data.

```{r }

distance <- dist(scaled.data, method="euclidean")

hc <- hclust(distance, method="complete")
plot(hc, cex=.6, hang=-1, main="Dendrogram")

fviz_nbclust(scaled.data, FUN = hcut, method = "wss")
fviz_nbclust(scaled.data, FUN = hcut, method = "silhouette")

group <- cutree(hc, k=4)
clust.data$HCgroup <- group

plot(clust.data$PC1, clust.data$PC2,
     pch = 19,
     col = factor(clust.data$HCgroup),
     xlab ="PC1",
     ylab = "PC2",
     main = "Hierarchical Clustering")





```
<h3 style="color:black;">Hierarchical Clustering Conclusion</h3>

The hierarchical clustering analysis reveals that the dataset naturally forms a small number of broad groups rather than many sharply separated clusters. The dendrogram shows long vertical merges, indicating that observations are generally similar to one another and only become distinguishable at higher distances. Both the WSS and silhouette methods suggest that around four clusters provide a reasonable balance between compactness and separation.

When these four groups are plotted on the PCA space, the clusters appear partially overlapping, which means the differences between groups are subtle rather than sharply defined. One cluster is more spread along the positive PC1 direction, while the others are concentrated near the center of the PCA plot. This suggests that the strongest separation is driven by only one or two underlying dimensions, with most observations sharing similar feature patterns.

Overall, hierarchical clustering indicates that the dataset contains a few loosely defined subgroups, with moderate separation along the principal components. The structure is not strongly partitioned, but there are identifiable regions in the data where observations tend to cluster together based on their similarity in the selected numerical attributes.